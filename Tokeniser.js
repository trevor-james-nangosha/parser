/**
 * Tokeniser class: responsible for pulling tokens from the stream generated by the parser.
 */
class Tokeniser {
    /**
     * the string gets passed into this function so that a stream of tokens is generated.
     * @param string
     */
    init(string) {
        this._string = string
        this._cursor = 0
    }

    /**
     * this function checks whether the cursor has reached the end of the file.
     * @returns 
     */
    isEOF() {
        return this._cursor === this._string.length
    }

    /**
     * this checks whether there are more tokens in the stream generated before the cursor can be moved.
     * @returns 
     */
    hasMoreTokens() {
        return this._cursor < this._string.length
    }

    /**
     * this returns the next token in the stream. If there is no token, it returns null
     * @returns 
     */
    getNextToken() {
        if (!this.hasMoreTokens()) {
            return null
        }

        const string = this._string.slice(this._cursor)
        if (!Number.isNaN(Number(string[0]))) {
            let number = ''
            while (!Number.isNaN(Number(string[this._cursor]))) {
                number += string[this._cursor++]
            }

            return {
                type: 'NUMBER',
                value: number,
            }
        }

    //String tokens
        if (string[0] === '"') {
            let s = ''
            do {
                s += string[this._cursor++]
            } while (string[this._cursor] !== '"' && !this.isEOF())
            s += this._cursor++ //skip
                return {
                    type: 'STRING',
                    value: s
                }
        }

        return null
    }
}

module.exports = {
    Tokeniser,
}