# PARSER

The purpose of this project is to create a parser from scratch.
In the parser below, we have a **real parser** and a **tokeniser**, both existing as separate modules.

The tokeniser module is responsible for generating a list of tokens given a stream, in this case a stream of text.
In the tokeniser module, use regular expressions for generating the rules of classifying different tokens. We store the 
different rules in the Spec array.
Our tokeniser can also recognise whitespaces and comments(single line comments and multi line comments), but we return null
for them ie. we do not have a token type for them.

The parser module gets the list of tokens generated by the tokeniser and then returns the type of token.
We shall be representing our different types of tokens as JSON objects.
## So far we have the following types of tokens,
* NumericLiteral
* StringLiteral

So far the way to use the parser is through the run.js file and id as follows;
You have to use `const parser = new Parser()` to import the Parser module, then continue as below.

```
const program = `some random stream`
const ast = parser.parse(program)
console.log(JSON.stringify(ast, null, 4))
```




